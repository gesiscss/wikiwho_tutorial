{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge all tokens with revisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from wikiwho_wrapper import WikiWho\n",
    "from os.path import exists\n",
    "ww = WikiWho()\n",
    "    \n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "article = \"John Logie Baird\"\n",
    "\n",
    "#filename = 'data/freddie_mercury.df'\n",
    "filename = f'data/{article}.df'\n",
    "try:\n",
    "    #raise Exception()\n",
    "    df = pd.read_pickle(filename)\n",
    "except:\n",
    "    df = ww.dv.all_content(f\"{article}\")\n",
    "    df = df[~((df['in'] == -1) & (df['out'] == -1))]\n",
    "    df = df.drop(columns=['article_title', 'page_id']) \n",
    "    df['action'] = 'rein'\n",
    "    df.loc[df['in']==-1, 'action'] = 'in'\n",
    "    df.loc[df['in']==-1, 'in'] = df.loc[df['in']==-1, 'o_rev_id']\n",
    "\n",
    "    revisions = ww.dv.rev_ids_of_article(f'{article}')\n",
    "\n",
    "    df = pd.merge(df,\n",
    "                  revisions.rename(\n",
    "                      columns={\n",
    "                          'rev_id': 'in',\n",
    "                          'o_editor': 'in_editor',\n",
    "                          'rev_time': 'in_rev_time'\n",
    "                      })[['in', 'in_editor', 'in_rev_time']],\n",
    "                  how='left', on='in')\n",
    "\n",
    "    df = pd.merge(df,\n",
    "                  revisions.rename(\n",
    "                      columns={\n",
    "                          'rev_id': 'out',\n",
    "                          'o_editor': 'out_editor',\n",
    "                          'rev_time': 'out_rev_time'\n",
    "                      })[['out', 'out_editor', 'out_rev_time']],\n",
    "                  how='left', on='out')\n",
    "\n",
    "\n",
    "    df.to_pickle(filename)\n",
    "\n",
    "    \n",
    "    \n",
    "# idea for a 2nd iteration, look for unrelated signs: parents, nationality, born\n",
    "india = ['Indian', 'Indians']\n",
    "persia = ['Parsi', 'Persian', 'Persians']\n",
    "azeri = ['Azeri']\n",
    "iran = ['iranian', 'iranians']\n",
    "brits = ['british']\n",
    "\n",
    "scottish = [\"scottish\"]\n",
    "british = [\"british\"]\n",
    "\n",
    "history = ww.dv.rev_ids_of_article(article=article).sort_values('rev_time')\n",
    "revisions = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_dt_replace(series, year=None, month=None, day=None):\n",
    "    return pd.to_datetime(\n",
    "        {'year': series.dt.year if year is None else year,\n",
    "         'month': series.dt.month if month is None else month,\n",
    "         'day': series.dt.day if day is None else day})\n",
    "\n",
    "def get_in_contexts(revid, tokenid):\n",
    "    global revisions\n",
    "    \n",
    "    try:\n",
    "        _df = revisions[revid]\n",
    "    except:\n",
    "        print(f'downloading {revid}')\n",
    "        _df = revisions[revid] = ww.dv.specific_rev_content_by_article_title(\n",
    "            article=article, rev_id=revid, out=False, _in=False)\n",
    "        \n",
    "    idx = _df[_df['token_id'] == tokenid].index[0]\n",
    "    \n",
    "    return ' '.join(_df.iloc[idx - 15 : idx ]['token']), ' '.join(_df.iloc[idx + 1 : idx + 15]['token'])\n",
    "    \n",
    "def get_out_contexts(revid, tokenid):\n",
    "    global revisions\n",
    "    try:\n",
    "        revid = history[history.shift(-1)['rev_id'] == revid]['rev_id'].iloc[0]\n",
    "    except Exception as e:\n",
    "        print('error')\n",
    "        print(revid)\n",
    "        print('error')\n",
    "        raise e\n",
    "    try:\n",
    "        _df = revisions[revid]\n",
    "    except:\n",
    "        print(f'downloading {revid}')\n",
    "        _df = revisions[revid] = ww.dv.specific_rev_content_by_article_title(article=article, rev_id=revid)\n",
    "        \n",
    "    idx = _df[_df['token_id'] == tokenid].index[0]\n",
    "    \n",
    "    return ' '.join(_df.iloc[idx - 15 : idx ]['token']), ' '.join(_df.iloc[idx + 1 : idx + 15]['token'])\n",
    "    \n",
    "    \n",
    "def search_token(df, tokens, _filter='duplicates', i_o='rein'):\n",
    "    ltokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    io_sel=None\n",
    "    if i_o == 'in':\n",
    "        io_sel = df[f'{i_o}'] == df[f'o_rev_id']\n",
    "    elif i_o == 'out':\n",
    "        io_sel = df[f'{i_o}'] != -1        \n",
    "    elif i_o == 'rein':\n",
    "        i_o = 'in'\n",
    "        io_sel = df[f'{i_o}'] != df[f'o_rev_id']\n",
    "  \n",
    "    _df = df.loc[(df['token'].isin(ltokens)) & io_sel, \n",
    "                 [f'{i_o}', 'token_id', f'{i_o}_rev_time', f'{i_o}_editor', 'token']]\n",
    "    \n",
    "    \n",
    "    _df[f'{i_o}_rev_time']= pd.to_datetime(_df[f'{i_o}_rev_time'])\n",
    "    _df = _df.sort_values([f'{i_o}_rev_time','token_id'], ascending=True)\n",
    "    _df['date'] = vec_dt_replace(_df[f'{i_o}_rev_time'], day=1)\n",
    "\n",
    "    if _filter is None:\n",
    "        pass\n",
    "    if _filter == 'uniques':\n",
    "        _df = _df.drop_duplicates(subset=f'{i_o}', keep=False)\n",
    "    elif _filter == 'duplicates':\n",
    "        _df = _df[_df.duplicated(subset=[f'{i_o}'], keep=False)]\n",
    "        \n",
    "    if i_o == 'in':\n",
    "        _df['left'], _df['right'] = zip(*_df[[f'{i_o}', 'token_id']].apply(\n",
    "            lambda x: get_contexts(revid = x[f'{i_o}'], tokenid = x['token_id']), axis=1))\n",
    "    elif i_o == 'out':\n",
    "        #prev = \n",
    "        _df['left'], _df['right'] = zip(*_df[[f'{i_o}', 'token_id']].apply(\n",
    "            lambda x: get_contexts(revid = history[history.shift(-1)['rev_id'] == x[f'{i_o}']]['rev_id'].iloc[0],\n",
    "                                   tokenid = x['token_id']), axis=1))\n",
    "\n",
    "    return _df.set_index(['date', f'{i_o}_rev_time'])\n",
    "\n",
    "\n",
    "\n",
    "def search_all_tokens(df, tokens, i_o=['in','out']):\n",
    "    \n",
    "    # make a recursive call\n",
    "    if not isinstance(i_o, str):\n",
    "        return pd.concat((search_all_tokens(df, tokens, i_o=io) for io in i_o), axis=0)\n",
    "    \n",
    "    ltokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    io_sel=None\n",
    "    coi = [f'{i_o}', 'token_id', f'{i_o}_rev_time', f'{i_o}_editor', 'token', 'action']\n",
    "    if i_o == 'out':\n",
    "        _df = df.loc[(df['token'].isin(ltokens)) & (df[f'{i_o}'] != -1), coi]\n",
    "        _df['action'] = 'out'\n",
    "    elif i_o == 'in':\n",
    "        _df = df.loc[(df['token'].isin(ltokens)), coi]\n",
    "        \n",
    "    _df = _df.rename(columns = {\n",
    "        f'{i_o}': 'rev_id', \n",
    "        f'{i_o}_rev_time': 'rev_time', \n",
    "        f'{i_o}_editor': 'editor',\n",
    "    })\n",
    "\n",
    "    _df[f'rev_time']= pd.to_datetime(_df[f'rev_time'])\n",
    "    _df = _df.sort_values(['token', f'rev_time','token_id'], ascending=True)\n",
    "    _df['date'] = vec_dt_replace(_df[f'rev_time'], day=1)\n",
    "    _df['duplicated'] = _df.duplicated(subset=[f'rev_id'], keep=False)\n",
    "    \n",
    "\n",
    "    if i_o == 'in':\n",
    "        _df['left'], _df['right'] = zip(*_df[[f'rev_id', 'token_id']].apply(\n",
    "            lambda x: get_in_contexts(revid = x[f'rev_id'], tokenid = x['token_id']), axis=1))\n",
    "    elif i_o == 'out':\n",
    "        #prev = \n",
    "        _df['left'], _df['right'] = zip(*_df[[f'rev_id', 'token_id']].apply(\n",
    "            lambda x: get_out_contexts(x[f'rev_id'], tokenid = x['token_id']), axis=1))\n",
    "\n",
    "    return _df.set_index(['date', f'rev_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rev_history(df, article, rev_id):\n",
    "    _df = ww.dv.specific_rev_content_by_article_title(article=article, rev_id=rev_id)\n",
    "    _df = pd.merge(\n",
    "        _df, df[['token_id', 'in', 'out']], \n",
    "        how='left', on='token_id')\n",
    "    _df = _df[(_df['in'] <= _df['rev_id'])]\n",
    "    _df.loc[_df['out'] > _df['rev_id'],'out'] = -1\n",
    "    return _df\n",
    "\n",
    "rev_history = get_rev_history(df, article, 58551262)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query all revisions that reinserted a token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = search_all_tokens(df, british + scottish, 'in')\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "display(HTML(_df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniques | none\n",
    "_df = search_token(df, scottish, None, 'in')\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "display(HTML(_df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query all revisions that removed a token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = search_all_tokens(df, british + scottish, 'out')\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "display(HTML(_df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniques | duplicates | none\n",
    "_df = search_token(df, british, 'duplicates', i_o='out')\n",
    "    \n",
    "from IPython.core.display import HTML\n",
    "display(HTML(_df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query all revisions that inserted, reinsert or removed tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = search_all_tokens(df, british + scottish)\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "display(HTML(_df.to_html()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
