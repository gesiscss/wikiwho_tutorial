{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "These notebook shows how to find all the revisions that involve a given word in a document. In practice, this means, all the times that word was inserted, reinserted or removed. It also locates the left and right context of the word in that given revision. \n",
    "\n",
    "\n",
    "The first step is selecting a page\n",
    "\n",
    "\n",
    "# Select a page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"John Logie Baird\"\n",
    "\n",
    "# This caches the revisions tokens, it is better not to overwrite to avoid\n",
    "# downloading the revisions multiple times.\n",
    "revisions_cache = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge all tokens with revisions\n",
    "\n",
    "Here, the ins and outs (columns `in` and `out` contains revisions id) of all_tokens of an article are merged with the revision in order to also have the revision timestamp and the editor of the revision. \n",
    "\n",
    "Note that the merge happens twice, one for the ins and one for the outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from wikiwho_wrapper import WikiWho\n",
    "from os.path import exists\n",
    "\n",
    "# this will print all the cases\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "# wikiwho instance\n",
    "ww = WikiWho()\n",
    "\n",
    "# get all the content\n",
    "df = ww.dv.all_content(f\"{article}\")\n",
    "\n",
    "# clean insertions that only happen once\n",
    "# df = df[((df['in'] == -1) & (df['out'] == -1))]\n",
    "\n",
    "# drop useless colummns\n",
    "df = df.drop(columns=['article_title', 'page_id']) \n",
    "\n",
    "# by default all insertions are consider reinsertions\n",
    "df['action'] = 'rein'\n",
    "\n",
    "# fix the above line, first insertions should be in\n",
    "df.loc[df['in']==-1, 'action'] = 'in'\n",
    "\n",
    "# place the o_rev_id as the real insertion\n",
    "df.loc[df['in']==-1, 'in'] = df.loc[df['in']==-1, 'o_rev_id']\n",
    "\n",
    "# get all the revision data\n",
    "revisions = ww.dv.rev_ids_of_article(f'{article}')\n",
    "\n",
    "# merge the in revisions ids with the revisions\n",
    "df = pd.merge(df,\n",
    "              revisions.rename(\n",
    "                  columns={\n",
    "                      'rev_id': 'in',\n",
    "                      'o_editor': 'in_editor',\n",
    "                      'rev_time': 'in_rev_time'\n",
    "                  })[['in', 'in_editor', 'in_rev_time']],\n",
    "              how='left', on='in')\n",
    "\n",
    "# merge the out revisions ids with the revisions\n",
    "df = pd.merge(df,\n",
    "              revisions.rename(\n",
    "                  columns={\n",
    "                      'rev_id': 'out',\n",
    "                      'o_editor': 'out_editor',\n",
    "                      'rev_time': 'out_rev_time'\n",
    "                  })[['out', 'out_editor', 'out_rev_time']],\n",
    "              how='left', on='out')\n",
    "\n",
    "\n",
    "# sort the revisions\n",
    "history = revisions.sort_values('rev_time')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define some methods\n",
    "\n",
    "This methods do the heavy lifting of the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rev_history(df, article, rev_id):\n",
    "    \"\"\"Get the actions performed until certain revision\"\"\"\n",
    "    \n",
    "    _df = ww.dv.specific_rev_content_by_article_title(article=article, rev_id=rev_id)\n",
    "    _df = pd.merge(\n",
    "        _df, df[['token_id', 'in', 'out']], \n",
    "        how='left', on='token_id')\n",
    "    _df = _df[(_df['in'] <= _df['rev_id'])]\n",
    "    _df.loc[_df['out'] > _df['rev_id'],'out'] = -1\n",
    "    return _df\n",
    "\n",
    "def create_download_link(df, filename, title = \"Download CSV file\", ):\n",
    "    \"\"\"Create csv and link to download the processed filed\"\"\"\n",
    "    df.to_csv(filename, sep = ';')\n",
    "    html = f'<a href=\"{filename}\" target=\"_blank\">{title}</a>'\n",
    "    return HTML(html)\n",
    "\n",
    "def vec_dt_replace(series, year=None, month=None, day=None):\n",
    "    \"\"\"This will generate a date from the above parameters. It is useful to create\n",
    "    indexes by month\"\"\"\n",
    "    return pd.to_datetime(\n",
    "        {'year': series.dt.year if year is None else year,\n",
    "         'month': series.dt.month if month is None else month,\n",
    "         'day': series.dt.day if day is None else day})\n",
    "\n",
    "def get_in_contexts(revid, tokenid):\n",
    "    \"\"\"get the left and right context of insertions and reinsertions\"\"\"\n",
    "    global revisions\n",
    "    \n",
    "    try:\n",
    "        _df = revisions_cache[revid]\n",
    "    except:\n",
    "        print(f'downloading {revid}')\n",
    "        _df = revisions_cache[revid] = ww.dv.specific_rev_content_by_article_title(\n",
    "            article=article, rev_id=revid, out=False, _in=False)\n",
    "        \n",
    "    idx = _df[_df['token_id'] == tokenid].index[0]\n",
    "    \n",
    "    return revid, idx, ' '.join(_df.iloc[idx - 15 : idx ]['token']), ' '.join(_df.iloc[idx + 1 : idx + 15]['token'])\n",
    "\n",
    "    \n",
    "def get_out_contexts(revid, tokenid):\n",
    "    \"\"\"get the left and right context of removes\"\"\"\n",
    "    global revisions\n",
    "\n",
    "    try:\n",
    "        revid = history[history.shift(-1)['rev_id'] == revid]['rev_id'].iloc[0]\n",
    "    except Exception as e:\n",
    "        print('error')\n",
    "        print(revid)\n",
    "        print('error')\n",
    "        raise e\n",
    "    try:\n",
    "        _df = revisions_cache[revid]\n",
    "    except:\n",
    "        print(f'downloading {revid}')\n",
    "        _df = revisions_cache[revid] = ww.dv.specific_rev_content_by_article_title(article=article, rev_id=revid)\n",
    "        \n",
    "    idx = _df[_df['token_id'] == tokenid].index[0]\n",
    "    \n",
    "    return revid, idx, ' '.join(_df.iloc[idx - 15 : idx ]['token']), ' '.join(_df.iloc[idx + 1 : idx + 15]['token'])\n",
    "    \n",
    "\n",
    "def search_all_tokens(df, tokens, i_o=['in','out'], with_context=False):\n",
    "    \"\"\"Search all times that actions were performed in the document\"\"\"\n",
    "    \n",
    "    # make a recursive call\n",
    "    if not isinstance(i_o, str):\n",
    "        return pd.concat((search_all_tokens(df, tokens, i_o=io, with_context=with_context) for io in i_o), axis=0)\n",
    "\n",
    "    ltokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    io_sel=None\n",
    "    coi = [f'{i_o}', 'token_id', f'{i_o}_rev_time', f'{i_o}_editor', 'token', 'action']\n",
    "    if i_o == 'out':\n",
    "        _df = df.loc[(df['token'].isin(ltokens)) & (df[f'{i_o}'] != -1), coi]\n",
    "        _df['action'] = 'out'\n",
    "    elif i_o == 'in':\n",
    "        _df = df.loc[(df['token'].isin(ltokens)), coi]\n",
    "        \n",
    "    _df = _df.rename(columns = {\n",
    "        f'{i_o}': 'rev_id', \n",
    "        f'{i_o}_rev_time': 'rev_time', \n",
    "        f'{i_o}_editor': 'editor',\n",
    "    })\n",
    "\n",
    "    _df[f'rev_time']= pd.to_datetime(_df[f'rev_time'])\n",
    "    _df = _df.sort_values(['token', f'rev_time','token_id'], ascending=True)\n",
    "    _df['date'] = vec_dt_replace(_df[f'rev_time'], day=1)\n",
    "    _df['duplicated'] = _df.duplicated(subset=[f'rev_id'], keep=False)\n",
    "    \n",
    "\n",
    "    if len(_df) > 0 and with_context:\n",
    "        if i_o == 'in':\n",
    "            _df['revid_ctxt'], _df['pos'], _df['left'], _df['right'] = zip(*_df[[f'rev_id', 'token_id']].apply(\n",
    "                lambda x: get_in_contexts(revid = x[f'rev_id'], tokenid = x['token_id']), axis=1))\n",
    "        elif i_o == 'out':\n",
    "            _df['revid_ctxt'], _df['pos'], _df['left'], _df['right'] = zip(*_df[[f'rev_id', 'token_id']].apply(\n",
    "                lambda x: get_out_contexts(x[f'rev_id'], tokenid = x['token_id']), axis=1))\n",
    "    else:\n",
    "        _df['left'] = ''\n",
    "        _df['right'] = ''\n",
    "        _df['pos'] = -1\n",
    "        _df['revid_ctxt'] = -1\n",
    "\n",
    "    return _df.set_index(['date', f'rev_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select the tokens that will be searched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [\"scottish\", \"british\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query all revisions that inserted, reinsert or removed tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = search_all_tokens(df, tokens, with_context=True)\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(create_download_link(_df, f'data/{article}.csv'))\n",
    "_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(revisions_cache[771998521].token)\n",
    "#revisions_cache[771998521]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
